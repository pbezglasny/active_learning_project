{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_metric\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "dataset = DatasetDict.load_from_disk('/home/pavel/work/active_learning_project/exploded_dataset')\n",
    "\n",
    "f1 = load_metric('f1')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric('f1')\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def preprocess_collator(batch):\n",
    "    print(batch)\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # pad inputs and labels\n",
    "    # print(features)\n",
    "    # data={'input_ids':batch['input_ids'], 'token_type_ids':batch['token_type_ids'], 'attention_mask':batch['attention_mask']}\n",
    "    batch = tokenizer.pad()\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# {TypeError}TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
    "def preprocess_function(examples):\n",
    "    # print(examples['dialog'])\n",
    "    # print(examples)\n",
    "    result = tokenizer(examples['dialog'], truncation=True, padding=True, max_length=512)\n",
    "    # print(result)\n",
    "    result['labels'] = examples['act']\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10897 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c7d6979551742a89437db2a92edebe6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/pavel/work/active_learning_project/exploded_dataset/test/cache-d30902102f2c3ebf.arrow\n",
      "Loading cached processed dataset at /home/pavel/work/active_learning_project/exploded_dataset/validation/cache-1cf20b8a7d425f03.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, batch_size=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'act': 4,\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'dialog': ' All right . ',\n 'dialog_num': 0,\n 'input_ids': [101,\n  2035,\n  2157,\n  1012,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'labels': 4,\n 'token_type_ids': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from transformers import get_scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "\n",
    "class DialogStats:\n",
    "    def __init__(self):\n",
    "        self.correct_ans = 0\n",
    "        self.total_ans = 0\n",
    "\n",
    "    def add_ans(self, correct):\n",
    "        self.total_ans += 1\n",
    "        if correct:\n",
    "            self.correct_ans += 1\n",
    "\n",
    "    @property\n",
    "    def ratio(self):\n",
    "        if self.total_ans == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.correct_ans / self.total_ans\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.correct_ans}/{self.total_ans}'\n",
    "\n",
    "\n",
    "class DialogPrediction:\n",
    "    def __init__(self):\n",
    "        self.answers = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.answers = defaultdict(lambda: DialogStats())\n",
    "\n",
    "    def add_answer(self, dialog_id, correct):\n",
    "        self.answers[dialog_id].add_ans(correct)\n",
    "\n",
    "    def get_bottom_k_percents(self, k):\n",
    "        answer = []\n",
    "        result_count = len(self.answers) * k // 100\n",
    "        result_count = max(result_count, 1)\n",
    "        for k, v in self.answers.items():\n",
    "            if len(answer) < result_count:\n",
    "                heapq.heappush(answer, (-v.ratio, k))\n",
    "            else:\n",
    "                prev_ratio, dialog_id = heapq.heappop(answer)\n",
    "                if prev_ratio > -v.ratio:\n",
    "                    heapq.heappush(answer, (prev_ratio, dialog_id))\n",
    "                else:\n",
    "                    heapq.heappush(answer, (-v.ratio, k))\n",
    "        return [dialog_id for _, dialog_id in answer]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.answers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class WorstDialogSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source,\n",
    "                 dialog_predictions: DialogPrediction,\n",
    "                 bottom_k_percents: int):\n",
    "        super().__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        self.dialog_prediction = dialog_predictions\n",
    "        self.full_length = len(data_source)\n",
    "        self.bottom_k_percents = bottom_k_percents\n",
    "        self.is_init = False\n",
    "        self.worst_dialog_ids = None\n",
    "        self.worst_dataset_indices = None\n",
    "\n",
    "    def set_init(self, is_init=True):\n",
    "        self.is_init = is_init\n",
    "\n",
    "    def choose_worst(self, bottom_k_percents=None):\n",
    "        if bottom_k_percents is None:\n",
    "            bottom_k_percents = self.bottom_k_percents\n",
    "        self.set_init(True)\n",
    "        self.worst_dialog_ids = set(self.dialog_prediction.get_bottom_k_percents(bottom_k_percents))\n",
    "        self.worst_dataset_indices = []\n",
    "\n",
    "        for i in range(len(self.data_source)):\n",
    "            d = self.data_source[i]\n",
    "            if d['dialog_num'] in self.worst_dialog_ids:\n",
    "                self.worst_dataset_indices.append(i)\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        if not self.is_init:\n",
    "            return iter(range(len(self.data_source)))\n",
    "        else:\n",
    "            return iter(self.worst_dataset_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        if not self.is_init:\n",
    "            return self.full_length\n",
    "        else:\n",
    "            return len(self.worst_dataset_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2724 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01353b8c80ff4169a0bbcbb86bb357ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.7953812032940314}\n",
      "2725\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n",
      "{'f1': 0.7988632639641498}\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "bottom_percents = 10\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "model.to(device)\n",
    "\n",
    "dp = DialogPrediction()\n",
    "\n",
    "train_worst_sampler = WorstDialogSampler(dataset['train'], dp, bottom_percents)\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=batch_size, sampler=train_worst_sampler)\n",
    "eval_dataloader = DataLoader(dataset['validation'], batch_size=batch_size)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_training_steps = (len(dataset['train']) + len(dataset)*bottom_percents //10 * (num_epochs-1)) // batch_size\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batches = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch_dict = {k: v for k, v in batch.items()}\n",
    "        data = tokenizer(batch_dict['dialog'], truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "        data['labels'] = batch_dict['act']\n",
    "        batch = {k: v.to(device) for k, v in data.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        if not train_worst_sampler.is_init:\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            for i in range(len(data['labels'])):\n",
    "                dp.add_answer(int(batch_dict['dialog_num'][i]), predictions[i] == data['labels'][i])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        batches += 1\n",
    "\n",
    "    if not train_worst_sampler.is_init:\n",
    "        train_worst_sampler.choose_worst()\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch_dict = {k: v for k, v in batch.items()}\n",
    "        data = tokenizer(batch_dict['dialog'], truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "        data['labels'] = batch_dict['act']\n",
    "        batch = {k: v.to(device) for k, v in data.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        f1.add_batch(predictions=predictions, references=data['labels'])\n",
    "    print(f1.compute(average='weighted'))\n",
    "    print(batches)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}